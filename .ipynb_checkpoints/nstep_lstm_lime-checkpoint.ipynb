{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer_lstm_blstm\n",
    "\n",
    "- 環境:Ubuntu18.04\n",
    "- GPU:GeFoce GTX 1080TI\n",
    "- ドライバー:NVIDIA-SMI 460.32.03, Driver Version: 460.32.03, CUDA Version: 11.2\n",
    "- 形態素解析:mecab-ipadic-neologd\n",
    "- 単語のベクトル化 (nwjc2vec):https://www.gsk.or.jp/catalog/gsk2020-d\n",
    "    - ./:2class_train.txt\n",
    "    - ./:2class_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chainer\n",
    "!pip install cupy-cuda101\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 必要なライブラリをインポート\n",
    "\n",
    "from pylab import *\n",
    "import os\n",
    "import argparse\n",
    "import codecs\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import chainer\n",
    "import chainer.optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "from chainer import Chain\n",
    "from chainer import training\n",
    "from chainer import reporter\n",
    "from chainer.training import extensions\n",
    "from chainer.training import triggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析（mecab-ipadic-neologd）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mecab -Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/ < 2class_train.txt > neologd_2class_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mecab -Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/ < 2class_test.txt > neologd_2class_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NWJC2vecの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vecの読み込み(word embedding用) \n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('/home/share/NWJC2vec/nwjc_word_skip_300_8_25_0_1e4_6_1_0_15.txt.vec', binary=False)\n",
    "#w2v = KeyedVectors.load_word2vec_format('../../Documents/nwjc_word_skip_300_8_25_0_1e4_6_1_0_15.txt.vec', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数\n",
    "\n",
    "def argument():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train_file', default=\"2class_useful_train.txt\")\n",
    "    parser.add_argument('--test_file', default=\"2class_useful_test.txt\")\n",
    "#    parser.add_argument('--train_file', default=\"shuffle_lstm_mecab_train_typhoon15.txt\")\n",
    "#    parser.add_argument('--test_file', default=\"shuffle_lstm_mecab_test_typhoon15.txt\")\n",
    "#    parser.add_argument('--train_file', default=\"lstm_train_typhoon15_neologd.txt\")\n",
    "#    parser.add_argument('--test_file', default=\"lstm_test_typhoon15_neologd.txt\")\n",
    "    parser.add_argument('--mode', default=\"BLSTM\") # LSTMかBLSTMか\n",
    "    parser.add_argument('--n_layers', default=1, type=int) # 層の数 BLSTMなら1で2つ分(ForwardとBackward)\n",
    "    parser.add_argument('--batchsize', default=2, type=int) # batchsize(学習に使うデータ数)\n",
    "    parser.add_argument('--embed', default=300, type=int) # w2vのベクトルの次元数\n",
    "    parser.add_argument('--hidden', default=300, type=int) # ユニット数\n",
    "#    parser.add_argument('--epoch', default=300, type=int) # epoch数(学習回数)\n",
    "    parser.add_argument('--epoch', default=20, type=int) # epoch数(学習回数)\n",
    "    parser.add_argument('--alpha', default=0.001, type=float) # 学習率\n",
    "    parser.add_argument('--drop', default=0.5, type=float) # dropout\n",
    "    parser.add_argument('--clipping', default=5.0, type=float) # 勾配の上限を設定\n",
    "    parser.add_argument('--save_dir', default='./result_blstm') # 保存するディレクトリ名\n",
    "    parser.add_argument('--model', default=\"ty15\") # 結果を保存するファイル名(topic)\n",
    "    parser.add_argument('--classes', default=2, type=int) # 分類する数\n",
    "    parser.add_argument('--use_gpu', default=1, type=int) # gpuの番号0-3\n",
    "# 使用しないパラメータ\n",
    "#    parser.add_argument('--maxlen', default=30, type=int) # 最長の長さ\n",
    "#    parser.add_argument('--vocab', default=3000, type=int) # 単語数\n",
    "#    parser.add_argument('--unchain', action='store_true', default=False) # \n",
    "#    args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argument() # 引数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果のディレクトリ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果ディレクトリ作成\n",
    "if os.path.isdir(args.save_dir) == False:\n",
    "    os.mkdir(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対象ファイル名メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load_data train   ***************************************\n",
    "#fname_tr=\"../LSTM/LSTM_same_wakati.txt\" #all\n",
    "#fname_tr=\"../LSTM/LSTM_same_train_wakati.txt\"\n",
    "#fname_tr=\"../LSTM/lstm_ooame_train_mecab.csv\"\n",
    "#fname_tr=\"../LSTM/lstm_taifu_train_mecab.csv\"\n",
    "#fname_tr=\"../LSTM/Netyu_LSTM_same_train_wakati.txt\"\n",
    "#fname_tr=\"../LSTM/Netyu_LSTM_same_wakati.txt\" #all\n",
    "#fname_tr=\"../LSTM/lstm_mecab_train_typhoon15_300.txt\"\n",
    "\n",
    "### load_data test  *************************************\n",
    "#fname=\"../LSTM/lstm_earthquake_test_mecab.csv\"\n",
    "#fname=\"../LSTM/lstm_ooame_test_mecab.csv\"\n",
    "#fname=\"../LSTM/lstm_taifu_test_mecab.csv\"\n",
    "#fname=\"../LSTM/lstm_mecab_test_typhoon15_30.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU or CPUの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの指定\n",
    "if 0 <= args.use_gpu <= 3:\n",
    "    xp = cuda.cupy\n",
    "    cuda.get_device(args.use_gpu).use()\n",
    "else:\n",
    "    xp = np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ランダムのシード値の固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同じ結果を出力するために乱数シードを固定\n",
    "\n",
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    xp.random.seed(seed)\n",
    "    if cuda.available:\n",
    "        cuda.cupy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed(0) # 乱数シード固定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMのモデル構築\n",
    "\n",
    "class LSTMBase(Chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        hx: Initial hidden states\n",
    "        cx: Initial cell states\n",
    "        xs: List of input sequences\n",
    "    \n",
    "    Returns:\n",
    "        hy: updated hidden states whose shape is the same as ``hx``\n",
    "                hxが更新されたもの\n",
    "        cy: updated cell states whose shape is the same as ``cx``\n",
    "                cxが更新されたもの\n",
    "        ys: ``ys[i]`` holds hidden states of the last layer corresponding to an input ``xs[i]``\n",
    "                各要素 `` ys [i] ``は入力 `` xs [i] ``に対応する最後の隠れ層の状態\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, embed_size, hidden_size, n_labels=2, dropout=0.5):\n",
    "        super(LSTMBase, self).__init__(\n",
    "            lstm=L.NStepLSTM(n_layers, embed_size, hidden_size, dropout),\n",
    "            linear=L.Linear(hidden_size, n_labels),\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.cx = self.hx = None\n",
    "        \n",
    "    def __call__(self, xs):\n",
    "        self.reset_state()\n",
    "        hy, cy, ys  = self.lstm(self.hx, self.cx, xs)\n",
    "        #y = self.linear(F.dropout(hy[0]))\n",
    "        y = self.linear(hy[0])\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLSTMモデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLSTMのモデル構築\n",
    "\n",
    "class BLSTMBase(Chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        hx: Initial hidden states\n",
    "        cx: Initial cell states\n",
    "        xs: List of input sequences\n",
    "    \n",
    "    Returns:\n",
    "        hy: updated hidden states whose shape is the same as ``hx``\n",
    "                hxが更新されたもの\n",
    "        cy: updated cell states whose shape is the same as ``cx``\n",
    "                cxが更新されたもの\n",
    "        ys: ``ys[i]`` holds hidden states of the last layer corresponding to an input ``xs[i]``\n",
    "                各要素 `` ys [i] ``は入力 `` xs [i] ``に対応する最後の隠れ層の状態\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, embed_size, hidden_size, n_labels=2, dropout=0.0):\n",
    "        super(BLSTMBase, self).__init__(\n",
    "            bi_lstm=L.NStepBiLSTM(n_layers, embed_size, hidden_size, dropout),\n",
    "            linear=L.Linear(hidden_size * 2, n_labels),\n",
    "        )\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.cx = self.hx = None\n",
    "        \n",
    "    def __call__(self, xs):\n",
    "        self.reset_state()\n",
    "        hy, cy, ys  = self.bi_lstm(self.hx, self.cx, xs)\n",
    "#        y = self.linear(F.dropout(F.concat([hy[0], hy[1]])))\n",
    "#        y = self.linear(F.dropout(hy[0]+hy[1])) # sum\n",
    "        y = self.linear(F.concat([hy[0], hy[1]])) # concat\n",
    "#        y = self.linear(F.dropout((hy[0]+hy[1])/2)) # average\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updaterの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このdeviceを受け取った時点でdeviceが@numpyというcpu.device型になっている\n",
    "class original_updater(training.StandardUpdater):\n",
    "    def __init__(self, train_iter, optimizer, device):\n",
    "        super(original_updater, self).__init__(\n",
    "            train_iter, optimizer, converter=convert, device=device)\n",
    "\n",
    "    # The core part of the update routine can be customized by overriding.\n",
    "    def update_core(self):\n",
    "        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n",
    "        # they are automatically named 'main'.\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        # Progress the dataset iterator for bprop_len words at each iteration.\n",
    "        # Get the next batch (a list of tuples of two word IDs)\n",
    "        batch = train_iter.__next__()\n",
    "        \n",
    "        # Concatenate the word IDs to matrices and send them to the device\n",
    "        # self.converter does this job\n",
    "        # (it is chainer.dataset.concat_examples by default)\n",
    "        xs, ts = self.converter(batch)\n",
    "        \n",
    "        # Compute the loss at this time step and accumulate it\n",
    "        loss = optimizer.target([x for x in xs], ts)\n",
    "\n",
    "        optimizer.target.cleargrads()  # Clear the parameter gradients\n",
    "        loss.backward()  # Backprop\n",
    "        optimizer.update()  # Update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数(soft_max_cross_entropy)の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_softmax_cross_entropy(ys, ts):\n",
    "    loss = 0\n",
    "    \n",
    "    loss = F.softmax_cross_entropy(ys, ts)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習するためのデータに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(x, t), (x, t) ...] -> ( [x, x, ...], [t, t, t])\n",
    "def convert(batch):\n",
    "    return tuple(([x for x, _ in batch], xp.array([y for _, y in batch])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluaterの設定(既存ではTypeErrorのため対応)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class original_Evaluator(training.extensions.Evaluator):\n",
    "\n",
    "    def __init__(self, iterator, target, device):\n",
    "        super(original_Evaluator, self).__init__(\n",
    "            iterator, target, converter=convert, device=device)\n",
    "\n",
    "    def evaluate(self):\n",
    "        iterator = self._iterators['main']\n",
    "        target = self._targets['main']\n",
    "        eval_func = self.eval_func or target\n",
    "\n",
    "        if self.eval_hook:\n",
    "            self.eval_hook(self)\n",
    "        it = copy.copy(iterator)\n",
    "        summary = reporter.DictSummary()\n",
    "\n",
    "        for batch in it:\n",
    "            observation = {}\n",
    "            with reporter.report_scope(observation):\n",
    "                xs, ts = self.converter(batch)\n",
    "                with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "                    eval_func([xp.array(x) for x in xs], ts)\n",
    "\n",
    "            summary.add(observation)\n",
    "\n",
    "        return summary.compute_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイルからデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,testデータの読み込み\n",
    "\n",
    "def load_data(fname):\n",
    "    print ('input file name:', fname)\n",
    "\n",
    "    \"\"\"\n",
    "    文書リストを作成(一致させる)\n",
    "    ex) document_list = [[word, word, ... , word], [word, ... , word], ... ]\n",
    "           target = [label, label, ... , label]\n",
    "    \"\"\"\n",
    "    target = [] #ラベル\n",
    "    source = [] #文書ベクトル\n",
    "    document_list = []\n",
    "    \n",
    "    fr = codecs.open(fname, \"r\", \"utf-8\", \"ignore\")\n",
    "    doc = fr.readlines()\n",
    "    fr.close()\n",
    "    \n",
    "    # 読み込んだデータをrandomする\n",
    "    # random.shuffle(doc)\n",
    "    \n",
    "    for l in doc:\n",
    "        sample = l.strip().split(' ',  1)\n",
    "        label = sample[0]\n",
    "        try:\n",
    "            document_list.append(sample[1]) #文書ごとの単語リスト\n",
    "            target.append(label) #ラベル\n",
    "        except:\n",
    "            print(\"load_data is error::  {} Line\".format(cnt))\n",
    "    return document_list,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章を単語単位に分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を単語に区切る\n",
    "\n",
    "def sentence2words(sentence):\n",
    "    sentence = sentence.replace(\"\\n\", \"\").replace(\"\\r\",\"\") # 改行削除\n",
    "#    sentence = re.sub(re.compile(r\"[!-\\/:-@[-`{-~]\"), \"\", sentence) # 記号削除 #をスペースに置き換え\n",
    "    sentence = sentence.split(\" \") # スペースで区切る\n",
    "    sentence_words = []\n",
    "    for word in sentence:\n",
    "#        if (re.compile(r\"^.*[0-9]+.*$\").fullmatch(word) is not None): # 数字が含まれるものは除外\n",
    "#            continue\n",
    "        sentence_words.append(word)\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単語のベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のベクトル化 ([\"地震\",\"は\"...] → tensor[tensor[],tensor[],...])\n",
    "\n",
    "def To_vec(args, line, xp):\n",
    "    x_batch = []\n",
    "    for x in line:\n",
    "        try:\n",
    "            x_batch.append(xp.array(w2v[x], dtype=xp.float32))\n",
    "        except:\n",
    "            if x==-1:\n",
    "                x_batch.append(xp.array([-1.0]*args.embed, dtype=xp.float32))\n",
    "            elif x==u\"NuLLL\":\n",
    "                x_batch.append(xp.array([-100.0]*args.embed, dtype=xp.float32))\n",
    "            else:\n",
    "                x_batch.append(xp.array([0.0]*args.embed, dtype=xp.float32))\n",
    "                \n",
    "    return x_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "\n",
    "def train():\n",
    "    print(\"-------Run-Training-------\")\n",
    "    print(\"\\n\")\n",
    "    print(args)\n",
    "    print(\"\\n\")\n",
    "    print(\"---load_data---\")\n",
    "    # ファイルの読み込み(train)\n",
    "    fname_train=args.train_file\n",
    "    train_x, train_t= load_data(fname_train)\n",
    "\n",
    "    print(\"---make_dataset---\")\n",
    "    # 文章を単語のリストにしてベクトル化(train)\n",
    "    train_x_vec=[] # 文書\n",
    "    for line_sentence in train_x:\n",
    "        words_list_train = sentence2words(line_sentence) # str\"word word ... word\" → list[word, word, ... , word]\n",
    "        sentence_vector_list_train = To_vec(args, words_list_train, xp) # list[word, word, ... , word] → np.array[[vector], [vector], ... , [vector]]\n",
    "        train_x_vec.append(xp.array(sentence_vector_list_train)) # np.array[[vector], [vector], ... , [vector]] → np.array[ [[vector], [vector], ... , [vector]], [[vector], [vector], ... , [vector]] ... ] ]\n",
    "    train_t = xp.array(train_t, dtype=\"int32\") # ラベル\n",
    "\n",
    "    # (データ, ラベル)のタプルデータセットを作成\n",
    "    dataset = chainer.datasets.TupleDataset(train_x_vec,train_t)\n",
    "    \n",
    "    # 学習データとバリデーションデータに分割\n",
    "    split_at = int(len(dataset) * 0.75) # 7.5:2.5に分割\n",
    "    train_data = dataset[0:split_at]\n",
    "    vali_data = dataset[split_at:len(dataset)]\n",
    "\n",
    "    # 使うデータセット\n",
    "    train_iter = chainer.iterators.SerialIterator(train_data, args.batchsize, repeat=True, shuffle=True)\n",
    "    vali_iter = chainer.iterators.SerialIterator(vali_data, args.batchsize, repeat=False, shuffle=False)\n",
    "    \n",
    "    print(\"---load_model---\")\n",
    "    print(\"      {}\".format(args.mode))\n",
    "    # 使用モデル(デフォルトでsoftmax_cross_entropy，loss_funcで損失関数を指定できる)\n",
    "    # Classifierでmodelをラップすることで、modelに損失の計算プロセスを追加します。 \n",
    "    # 引数に損失関数を指定しない場合は、softmax_cross_entropyを使います。 \n",
    "    if args.mode == \"LSTM\":\n",
    "        lstm = LSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    elif args.mode == \"BLSTM\":\n",
    "        lstm = BLSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    else:\n",
    "        raise Exception(\"Input mode \\\"LSTM\\\" or \\\"BLSTM\\\".\")\n",
    "    m = L.Classifier(lstm, lossfun=sum_softmax_cross_entropy)\n",
    "\n",
    "    # モデルに対してGPU使用\n",
    "    if 0 <= args.use_gpu <= 3:\n",
    "        m.to_gpu()\n",
    "        print(\"---use_gpu_{}---\".format(args.use_gpu))\n",
    "    else:\n",
    "        print(\"---use_cpu---\")\n",
    "    \n",
    "    print(\"---optimizer_Adam---\")\n",
    "    # 学習\n",
    "    # Optimizer\n",
    "    opt = chainer.optimizers.Adam(args.alpha) # 選択\n",
    "    opt.setup(m) # modelセット\n",
    "    opt.add_hook(chainer.optimizer.GradientClipping(args.clipping))# 勾配の上限を設定\n",
    "    \n",
    "    print(\"---updater_standardupdater---\")\n",
    "    # Updater originalを指定\n",
    "    updater = original_updater(train_iter, opt, device=args.use_gpu)\n",
    "    \n",
    "    print(\"---training---\")\n",
    "    snapshot_name = '{}_snapshot_epoch-'.format(args.model)\n",
    "    model_name = '{}_model_epoch-'.format(args.model)\n",
    "    loss_png = '{}_loss.png'.format(args.model)\n",
    "    acc_png = '{}_accuracy.png'.format(args.model)\n",
    "    # Trainer\n",
    "    # trainerの宣言\n",
    "    trainer = training.Trainer(updater,(args.epoch,'epoch'),out=args.save_dir)\n",
    "    # 学習の経過をtrainerのoutで指定したフォルダにlogというファイル名で記録する\n",
    "    trainer.extend(extensions.LogReport(trigger=(1, 'epoch')))\n",
    "    # Test-accuracy が更新されたときにモデルを保存する\n",
    "    # 定期的に状態をシリアライズ(保存)する\n",
    "    trigger = triggers.MaxValueTrigger('validation/main/accuracy', trigger=(1, 'epoch'))\n",
    "    trainer.extend(extensions.snapshot(filename=snapshot_name+'{.updater.epoch}'))\n",
    "    trainer.extend(extensions.snapshot_object(m, filename=model_name+'{.updater.epoch}'), trigger=trigger)\n",
    "    # バリデーションデータを使ってモデルの評価を行う\n",
    "    trainer.extend(original_Evaluator(vali_iter, m, device=args.use_gpu))\n",
    "    # 損失関数の値をグラフにする\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name=loss_png))\n",
    "    # 正答率をグラフにする\n",
    "    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name=acc_png))\n",
    "    # １エポックごと（trigger）に、trainデータに対するlossと、testデータに対するloss、経過時間（elapsed_time）を標準出力させる\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    # 進捗bar(今何%的な)\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "    # グラフをDOT Languageで描画してくれる\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    \n",
    "    # 実行\n",
    "    trainer.run()\n",
    "    \n",
    "    # 最終的な学習済みモデルを保存\n",
    "    chainer.serializers.save_npz(\"{}/{}_mymodel.npz\".format(args.save_dir, args.model), m)\n",
    "\n",
    "    print(\"--------Finish Training!----------\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_array = [] # LIME用 配列\n",
    "\n",
    "# 評価\n",
    "def predict():\n",
    "    print(\"-------Run-Predicting-------\")\n",
    "    print(\"\\n\")\n",
    "    print(args)\n",
    "    print(\"\\n\")\n",
    "    print(\"---load_data---\")\n",
    "    # ファイルの読み込み(test)\n",
    "    fname_test=args.test_file\n",
    "    test_x, test_t= load_data(fname_test)\n",
    "\n",
    "    print(\"---make_dataset---\")\n",
    "    # 文章を単語のリストにしてベクトル化(test)\n",
    "    test_x_vec=[]# 文書\n",
    "    for line_sentence in test_x:\n",
    "        words_list_test = sentence2words(line_sentence) # str\"word word ... word\" → list[word, word, ... , word]\n",
    "        sentence_vector_list_test = To_vec(args, words_list_test, xp) # list[word, word, ... , word] → np.array[[vector], [vector], ... , [vector]]\n",
    "        test_x_vec.append(xp.array(sentence_vector_list_test)) # np.array[[vector], [vector], ... , [vector]] → np.array[ [[vector], [vector], ... , [vector]], [[vector], [vector], ... , [vector]] ... ] ]\n",
    "    test_t = xp.array(test_t, dtype=\"int32\") # ラベル\n",
    "    \n",
    "    # (データ, ラベル)のタプルデータセットを作成\n",
    "    #dataset = chainer.datasets.TupleDataset(test_x_vec, test_t)\n",
    "    \n",
    "    # 使うデータセット\n",
    "    #test_iter = chainer.iterators.SerialIterator(dataset, args.batchsize, repeat=True, shuffle=True)\n",
    "\n",
    "    print(\"---load_model---\")\n",
    "    print(\"      {}\".format(args.mode))\n",
    "    # 使用モデル(デフォルトでsoftmax_cross_entropy，loss_funcで損失関数を指定できる)\n",
    "    # Classifierでmodelをラップすることで、modelに損失の計算プロセスを追加します。 \n",
    "    # 引数に損失関数を指定しない場合は、softmax_cross_entropyを使います。 \n",
    "    if args.mode == \"LSTM\":\n",
    "        lstm = LSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    elif args.mode == \"BLSTM\":\n",
    "        lstm = BLSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    else:\n",
    "        raise Exception(\"Input mode \\\"LSTM\\\" or \\\"BLSTM\\\".\")\n",
    "    m = L.Classifier(lstm, lossfun=sum_softmax_cross_entropy)\n",
    "    \n",
    "    # モデルに対してGPU使用\n",
    "    if 0 <= args.use_gpu <= 3:\n",
    "        m.to_gpu()\n",
    "        print(\"---use_gpu_{}---\".format(args.use_gpu))\n",
    "    else:\n",
    "        print(\"---use_cpu---\")\n",
    "    \n",
    "    print(\"---load_trained_model---\")\n",
    "    # 学習データを読み込む\n",
    "#    use_model = \"/{}_snapshot_epoch-300\".format(args.model)\n",
    "    load_model = \"{}/{}_mymodel.npz\".format(args.save_dir, args.model)\n",
    "    #chainer.serializers.load_npz(load_model, m, strict=False, path='updater/model:main/predictor/') # snapshot を読み込む場合はpathの設定が必要(snapshotは色々保存されてるから)\n",
    "    chainer.serializers.load_npz(load_model, m) # snapshot を読み込む場合はpathの設定が必要(snapshotは色々保存されてるから)\n",
    "    \n",
    "    # 結果\n",
    "    pos_num = 0\n",
    "    neg_num = 0\n",
    "    TP=0\n",
    "    FN=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "\n",
    "    judged=0\n",
    "    cnt=0\n",
    "    # auc_roc curve\n",
    "    y_test=list() # ラベル\n",
    "    prob=list() # +の確率\n",
    "    \n",
    "    # 結果の出力(ラベルとtweet)\n",
    "    tweet_data = codecs.open(\"{}/res_all_{}\".format(args.save_dir, fname_test).replace(\".txt\",\".csv\"),'w', 'sjis', 'ignore')\n",
    "    # 結果の出力(AUC)\n",
    "    tweet = codecs.open(\"{}/res_auc_{}\".format(args.save_dir, fname_test).replace(\".txt\",\".csv\"), 'w', 'sjis', 'ignore')\n",
    "    \n",
    "    print(\"---predict---\\n\")\n",
    "    # 1行ずつ評価\n",
    "    for x,t in zip(test_x_vec, test_t):\n",
    "        xs = []\n",
    "        xs.append(x)\n",
    "        # 評価\n",
    "        # モデルのforward関数に渡す\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False): # 学習しない設定にして無駄な計算を省く\n",
    "            y = m.predictor(xs)\n",
    "\n",
    "        # 1 or 0を出力\n",
    "        output = F.softmax(y, axis=1)\n",
    "\n",
    "        # 結果集計・書き込み\n",
    "        judged=xp.argmax(output.data)\n",
    "        tmp=list(output.data)\n",
    "        judged=xp.argmax(tmp[0])\n",
    "\n",
    "        tweet_data.write(\"{},{},{},{},{}\\n\".format(str(t), str(judged), str(tmp[0][1]), str(tmp[0][0]), test_x[cnt].replace(' ', '')))\n",
    "        tweet.write(\"{},{},{}\\n\".format(str(t), str(tmp[0][1]), str(tmp[0][0])))\n",
    "        predict_array.append(tmp[0]) # LIME用\n",
    "        \n",
    "        y_test.append(int(str(t))) # AUC 実際のラベル\n",
    "        prob.append(float(tmp[0][1])) # AUC+の値\n",
    "\n",
    "        if(judged==1 and t==1):\n",
    "            pos_num += 1\n",
    "            TP=TP+1\n",
    "        if(judged==1 and t==0):\n",
    "            neg_num += 1\n",
    "            FP=FP+1\n",
    "        if(judged==0 and t==1):\n",
    "            pos_num += 1\n",
    "            FN=FN+1\n",
    "        if(judged==0 and t==0):\n",
    "            neg_num += 1\n",
    "            TN=TN+1\n",
    "        cnt=cnt+1\n",
    "        if cnt % 100 == 0:\n",
    "            print(\"test_data:  {} / {}\".format(cnt, len(test_x)))\n",
    "        \n",
    "    tweet.close()\n",
    "    tweet_data.close()\n",
    "    print(\"ok\")\n",
    "\n",
    "    # 最終的な結果の出力\n",
    "    #適合率\n",
    "    if(TP+FN)!=0:\n",
    "        precision = (TP / (TP+FP))\n",
    "    else:\n",
    "        precision = 0\n",
    "    print(u\"適合率：\" + str(precision))\n",
    "\n",
    "    # 再現率\n",
    "    if(TP+FN)!=0:\n",
    "        recall = TP / (TP+FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "    print(u\"再現率：\"+str(recall))\n",
    "\n",
    "    # F値\n",
    "    if (TP+FN) !=0 and (TP+FN) !=0:\n",
    "        f_measure = (2*precision*recall) / (precision+recall)\n",
    "    else:\n",
    "        f_measure = 0\n",
    "    print(u\"F値：\"+str(f_measure))\n",
    "\n",
    "    # 精度\n",
    "    accuracy = (TP+TN) / (TP+FP+TN+FN)\n",
    "    print(u\"精度\"+str(accuracy))\n",
    "\n",
    "    # AUC Curve\n",
    "    y_test=np.array(y_test)\n",
    "    prob=np.array(prob)\n",
    "    fpr, tpr, thresholds= roc_curve(y_test, prob ,pos_label=1)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot( fpr,tpr)\n",
    "    plt.title(\"ROC curve : \"+args.model)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.grid(which='major',color='black',linestyle='-')\n",
    "    plt.grid(which='minor',color='black',linestyle='-')\n",
    "\n",
    "    # AUCの算出\n",
    "    #precision, recall, thresholds = precision_recall_curve(y_test, prob,pos_label=1)\n",
    "    #print(\"precision: {0}, recall: {1}, thresholds: {2}\".format(precision, recall, thresholds))\n",
    "    area = auc(fpr, tpr)\n",
    "    print(\"Area Under Curve: {0:.5f}\".format(area))\n",
    "    plt.savefig(\"{}/{}.png\".format(args.save_dir, area))\n",
    "\n",
    "    print(\"Eval data.\")\n",
    "    print(\"        判定結果\")\n",
    "    print(\"         1|    0\")\n",
    "    print(\"正解  1|\"+str(TP)+\"  \"+str(FN))\n",
    "    print(\"      0|\"+str(FP)+\"  \"+str(TN))\n",
    "\n",
    "    fw = codecs.open(\"{}/res_confusion_{}\".format(args.save_dir, fname_test).replace(\".txt\",\".csv\"), \"w\", \"sjis\", \"ignore\")\n",
    "    fw.write(\"データ数,正例,負例,適合率,再現率,F値,精度\\n\")\n",
    "    fw.write(\"{0},{1},{2},{3},{4},{5},{6}\\n\".format(TP+FN+FP+TN, pos_num, neg_num, precision, recall, f_measure, accuracy))\n",
    "    fw.write(\"\\n\")\n",
    "    fw.write(\",LSTM_pos,LSTM_neg\\n\")\n",
    "    fw.write(\"Correct_pos,{0},{1}\\n\".format(TP, FN))\n",
    "    fw.write(\"Correct_neg,{0},{1}\\n\".format(FP,TN))\n",
    "\n",
    "    fw.close()\n",
    "    print(\"--------Finish predict!----------\")    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "def main():\n",
    "    train()\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME\n",
    "- chainer, w2vに対応させるために変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "from lime.lime_text import LimeTextExplainer, IndexedString, IndexedCharacters, TextDomainMapper\n",
    "from lime import explanation, lime_base\n",
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.utils import check_random_state\n",
    "import pandas as pd\n",
    "\n",
    "class w2v_Lime(LimeTextExplainer):\n",
    "    def __init__(self, class_names=None, split_expression=None, bow=False, random_state=0):\n",
    "        super(w2v_Lime, self).__init__(\n",
    "#            kernel_width=25,\n",
    "#             kernel=None,\n",
    "#             verbose=False,\n",
    "             class_names=class_names,\n",
    "#             feature_selection='auto',\n",
    "             split_expression=split_expression,\n",
    "             bow=bow,\n",
    "#             mask_string=None,\n",
    "             random_state=random_state,\n",
    "#             char_level=False)\n",
    "        )\n",
    "    \n",
    "    def explain_instance(self,\n",
    "                     text_instance,\n",
    "                     classifier_fn,\n",
    "                     labels=(1,),\n",
    "                     top_labels=None,\n",
    "                     num_features=10,\n",
    "                     num_samples=5000,\n",
    "                     distance_metric='cosine',\n",
    "                     model_regressor=None):\n",
    "\n",
    "        # LIME用のindexを作成\n",
    "        indexed_string = (IndexedCharacters(\n",
    "            text_instance, bow=self.bow, mask_string=self.mask_string)\n",
    "                          if self.char_level else\n",
    "                          IndexedString(text_instance, bow=self.bow,\n",
    "                                        split_expression=self.split_expression,\n",
    "                                        mask_string=self.mask_string))\n",
    "        domain_mapper = TextDomainMapper(indexed_string)\n",
    "        \n",
    "        # データdataと評価時のsoftmaxの値yssとコサイン類似度の距離distancesを取得\n",
    "        data, yss, distances = self.__data_labels_distances(\n",
    "            indexed_string, classifier_fn, num_samples,\n",
    "            distance_metric=distance_metric)\n",
    "        \n",
    "        # class_nameが無ければ適当に決定する\n",
    "        if self.class_names is None:\n",
    "            self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "            \n",
    "        # Explanation class, with visualization functions. の作成\n",
    "        ret_exp = explanation.Explanation(domain_mapper=domain_mapper,\n",
    "                                          class_names=self.class_names,\n",
    "                                          random_state=self.random_state)\n",
    "        ret_exp.predict_proba = yss[0]\n",
    "        if top_labels:\n",
    "            labels = np.argsort(yss[0])[-top_labels:]\n",
    "            ret_exp.top_labels = list(labels)\n",
    "            ret_exp.top_labels.reverse()\n",
    "        for label in labels:\n",
    "            (ret_exp.intercept[label],\n",
    "             ret_exp.local_exp[label],\n",
    "             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data(\n",
    "                data, yss, distances, label, num_features,\n",
    "                model_regressor=model_regressor,\n",
    "                feature_selection=self.feature_selection)\n",
    "        return ret_exp\n",
    "    \n",
    "    #データdataと評価時のsoftmaxの値yssとコサイン類似度の距離distancesを取得\n",
    "    def __data_labels_distances(self,\n",
    "                            indexed_string,\n",
    "                            classifier_fn,\n",
    "                            num_samples,\n",
    "                            distance_metric='cosine'):\n",
    "        def distance_fn(x):\n",
    "            return sklearn.metrics.pairwise.pairwise_distances(\n",
    "                x, x[0], metric=distance_metric).ravel() * 100\n",
    "\n",
    "        doc_size = indexed_string.num_words()\n",
    "        sample = self.random_state.randint(1, doc_size + 1, num_samples - 1)\n",
    "        data = np.ones((num_samples, doc_size))\n",
    "        data[0] = np.ones(doc_size)\n",
    "        features_range = range(doc_size)\n",
    "        inverse_data = [indexed_string.raw_string()]\n",
    "        for i, size in enumerate(sample, start=1):\n",
    "            inactive = self.random_state.choice(features_range, size,\n",
    "                                                replace=False)\n",
    "            data[i, inactive] = 0\n",
    "            inverse_data.append(indexed_string.inverse_removing(inactive))\n",
    "            \n",
    "        # 既存からの変更点\n",
    "        # 文章を単語のリストにしてベクトル化(test)\n",
    "        inverse_data_2 = []# 文書\n",
    "        #print(inverse_data)\n",
    "        for line_sentence in inverse_data:\n",
    "            words_list_test = sentence2words(line_sentence) # str\"word word ... word\" → list[word, word, ... , word]\n",
    "            sentence_vector_list_test = To_vec(args, words_list_test, xp) # list[word, word, ... , word] → np.array[[vector], [vector], ... , [vector]]\n",
    "            inverse_data_2.append(xp.array(sentence_vector_list_test)) # np.array[[vector], [vector], ... , [vector]] → np.array[ [[vector], [vector], ... , [vector]], [[vector], [vector], ... , [vector]] ... ] ]\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False): # 学習しない設定にして無駄な計算を省く\n",
    "            output = F.softmax(classifier_fn.predictor(inverse_data_2), axis=1)\n",
    "        labels = chainer.cuda.to_cpu(output.data) # Variableからnp.arrayへ\n",
    "#        labels[0][0], labels[0][1]= labels[0][1], labels[0][0] # 逆順\n",
    "#        print(\"labels:\\n{}\".format(labels))\n",
    "        distances = distance_fn(sp.sparse.csr_matrix(data))\n",
    "\n",
    "        return data, labels, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lime():\n",
    "    sav_dir = \"lime_html_lstm_shuffle\"\n",
    "    # 結果ディレクトリ作成\n",
    "    if os.path.isdir(sav_dir) == False:\n",
    "        os.mkdir(sav_dir)\n",
    "\n",
    "    print(\"-------Run-LIME-------\")\n",
    "    print(\"\\n\")\n",
    "    print(\"---load_data---\")\n",
    "    # ファイルの読み込み(test)\n",
    "    fname_test=args.test_file\n",
    "    test_x, test_t= load_data(fname_test)\n",
    "\n",
    "    print(\"---load_model---\")\n",
    "    print(\"      {}\".format(args.mode))\n",
    "    # 使用モデル(デフォルトでsoftmax_cross_entropy，loss_funcで損失関数を指定できる)\n",
    "    # Classifierでmodelをラップすることで、modelに損失の計算プロセスを追加します。 \n",
    "    # 引数に損失関数を指定しない場合は、softmax_cross_entropyを使います。 \n",
    "    if args.mode == \"LSTM\":\n",
    "        lstm = LSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    elif args.mode == \"BLSTM\":\n",
    "        lstm = BLSTMBase(args.n_layers, args.embed, args.hidden, args.classes, args.drop)\n",
    "    else:\n",
    "        raise Exception(\"Input mode \\\"LSTM\\\" or \\\"BLSTM\\\".\")\n",
    "    m = L.Classifier(lstm, lossfun=sum_softmax_cross_entropy)\n",
    "\n",
    "    # モデルに対してGPU使用\n",
    "    if 0 <= args.use_gpu <= 3:\n",
    "        m.to_gpu()\n",
    "        print(\"---use_gpu_{}---\".format(args.use_gpu))\n",
    "    else:\n",
    "        print(\"---use_cpu---\")\n",
    "\n",
    "    print(\"---load_trained_model---\")\n",
    "    # 学習データを読み込む\n",
    "    load_model = \"{}/{}_mymodel.npz\".format(args.save_dir, args.model)\n",
    "    chainer.serializers.load_npz(load_model, m) # snapshot を読み込む場合はpathの設定が必要(snapshotは色々保存されてるから)\n",
    "\n",
    "    # We choose a sample from test set\n",
    "#    idx = 5 # 最終的には全て行う\n",
    "    for idx in range(len(test_x)):\n",
    "#    for idx in range(2):\n",
    "    \n",
    "        text_sample = test_x[idx] # 元データならtest_x(1-line 1-tweet)\n",
    "        class_names = ['0', '1']\n",
    "\n",
    "        # Lime\n",
    "        explainer = w2v_Lime(class_names=class_names, split_expression=r'\\W+', bow=False, random_state=0)\n",
    "        explanation = explainer.explain_instance(text_sample, m, num_features=1000, num_samples=10000)\n",
    "\n",
    "        weights = OrderedDict(explanation.as_list()) # 順列辞書にリストにする関数を\n",
    "    #    print(explanation.as_list())\n",
    "    #    lime_weights = pd.DataFrame({'words': list(weights.keys()), 'weights': list(weights.values())})\n",
    "\n",
    "    #    sns.barplot(x=\"words\", y=\"weights\", data=lime_weights);\n",
    "    #    plt.xticks(rotation=1)\n",
    "    #    plt.title('Tweet No.{} features weights given by LIME'.format(idx+1));\n",
    "#        explanation.show_in_notebook(text=True)\n",
    "        explanation.save_to_file(file_path=\"./{}/save_lstm_10000_{}.html\".format(sav_dir, idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle data (文脈考慮できているかどうか)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "neologd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epoch 20 Dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
